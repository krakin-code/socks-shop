import requests
from bs4 import BeautifulSoup
import re
import os
import time
import pandas as pd
from concurrent.futures import ThreadPoolExecutor

BASE_URL = "https://bankrot.cdtrf.ru/public/undef/card/tradel.aspx"
DETAIL_URL = "https://bankrot.cdtrf.ru/public/undef/card/trade.aspx?id={}"  # –î–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
SAVE_PATH = "data/parsed"
os.makedirs(SAVE_PATH, exist_ok=True)


def get_viewstate_params(session):
    """ –ü–æ–ª—É—á–∞–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã """
    response = session.get(BASE_URL, headers=HEADERS)
    soup = BeautifulSoup(response.text, 'html.parser')

    return {
        '__VIEWSTATE': soup.find('input', {'id': '__VIEWSTATE'})['value'],
        '__EVENTVALIDATION': soup.find('input', {'id': '__EVENTVALIDATION'})['value'],
        '__VIEWSTATEGENERATOR': soup.find('input', {'id': '__VIEWSTATEGENERATOR'})['value'],
    }


def get_page(session, page_number):
    """ –ü–æ–ª—É—á–∞–µ—Ç HTML-–∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å —Ç–æ—Ä–≥–∞–º–∏ """
    params = get_viewstate_params(session)
    params.update({
        '__EVENTTARGET': 'ctl00$MainContent$gvTradeL',
        'ctl00$MainContent$tbSearch': '',
        'ctl00$MainContent$ddlTradeLType': '-1',
        'ctl00$MainContent$ddlTradeLStatus': '-1',
        'ctl00$cph1$pgvTrades$ctl22$ddlPager': str(page_number)
    })
    response = session.post(BASE_URL, headers=HEADERS, data=params)
    return response.text


def extract_trade_codes(html):
    """ –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–¥—ã —Ç–æ—Ä–≥–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã """
    soup = BeautifulSoup(html, 'html.parser')
    table = soup.find('table', {'id': 'ctl00_cph1_pgvTrades'})
    if not table:
        return []
    rows = table.find_all('tr')[2:]
    return [row.find_all('td')[2].text.strip() for row in rows if len(row.find_all('td')) == 14]


def get_trade_data(trade_id):
    """ –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç–æ–≤–∞—Ä–∞ –∏ –∏—â–µ—Ç –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ """
    url = DETAIL_URL.format(trade_id)
    response = requests.get(url, headers=HEADERS)
    soup = BeautifulSoup(response.text, "html.parser")
    table = soup.find('table', class_="product-table-basic")
    if not table:
        return None

    for row in table.find_all('tr'):
        for cell in row.find_all('td'):
            text = cell.text.strip().lower()
            if '–∞–≤—Ç–æ–º–æ–±–∏–ª—å' in text:
                match = re.search(r'(\d+)\s+(–≥–æ–¥[–∞]?)', text) or re.search(r'(\d+)\s+(–≥\.–≤\.)', text)
                year = int(match.group(1)) if match else 0
                if year >= 2010 or year == 0:
                    return {"url": url, "description": text, "year": year}
    return None


def main(start_page=1, end_page=5):
    """ –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü """
    session = requests.Session()
    trade_ids = []

    print(f"üîç –ü–æ–ª—É—á–∞–µ–º —Ç–æ—Ä–≥–æ–≤—ã–µ –∫–æ–¥—ã —Å {start_page} –ø–æ {end_page} —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
    for page in range(start_page, end_page + 1):
        html = get_page(session, page)
        trade_ids.extend(extract_trade_codes(html))
        time.sleep(1)

    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(trade_ids)} —Ç–æ—Ä–≥–æ–≤.")

    print("üöó –ò—â–µ–º –∞–≤—Ç–æ–º–æ–±–∏–ª–∏ 2010+ –≥–æ–¥–∞...")
    results = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        for data in executor.map(get_trade_data, trade_ids):
            if data:
                results.append(data)

    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π.")

    df = pd.DataFrame(results)
    df.to_csv(os.path.join(SAVE_PATH, "cars.csv"), index=False, encoding="utf-8-sig")
    print(f"üìÇ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {SAVE_PATH}/cars.csv")


if __name__ == "__main__":
    main(1, 5)  # –£–∫–∞–∂–∏—Ç–µ –Ω—É–∂–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω —Å—Ç—Ä–∞–Ω–∏—Ü
